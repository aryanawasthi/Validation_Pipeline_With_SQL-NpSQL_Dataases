{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/08/87b404b8b3255d46caf0ecdccf871d501a2b58da9b844d3f9710ce9d4d53/pyspark-3.3.2.tar.gz (281.4MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4MB 3.8MB/s eta 0:00:013  |▊                               | 6.2MB 3.9MB/s eta 0:01:11     |████████████████████████████    | 245.3MB 3.6MB/s eta 0:00:11\n",
      "\u001b[?25hCollecting py4j==0.10.9.5 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/ec/60880978512d5569ca4bf32b3b4d7776a528ecf4bca4523936c98c92a3c8/py4j-0.10.9.5-py2.py3-none-any.whl (199kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/aryan/snap/jupyter/6/.cache/pip/wheels/a0/cd/51/9c10e56c6e5aabd71b659e6bf93e26f0bf85aa8c6364514deb\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7dd3504c366f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 18:20:58 WARN Utils: Your hostname, aryan-HP-EliteBook-Folio-9470m resolves to a loopback address: 127.0.1.1; using 192.168.29.242 instead (on interface wlo1)\n",
      "23/04/08 18:20:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 18:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"first_Application\").master(\"local[*]\").master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.242:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>first_Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f028f3ba680>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json=spark.read.format(\"json\").option(\"multiline\",True).option(\"path\",\"/home/aryan/Music/YouTube_Analysis_Data/CA_category_id.json\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etag: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- etag: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- kind: string (nullable = true)\n",
      " |    |    |-- snippet: struct (nullable = true)\n",
      " |    |    |    |-- assignable: boolean (nullable = true)\n",
      " |    |    |    |-- channelId: string (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |-- kind: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_json.selct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=data_json.select(\"items\")\n",
    "\n",
    "\n",
    "\"\"\"[Row(items=[Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/Xy1mB4_yLrHy_BmKmPBggty2mZQ\"', id='1', \n",
    "kind='youtube#videoCategory', snippet=Row(assignable=True, channelId='UCBR8-60-B28hp2BmDPdntcQ',\n",
    "title='Film & Animation')), Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"', id='2', \n",
    "kind='youtube#videoCategory', snippet=Row(assignable=True, channelId='UCBR8-60-B28hp2BmDPdntcQ', \n",
    "title='Autos & Vehicles')), Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/nqRIq97-xe5XRZTxbknKFVe5Lmg\"', id='10', \n",
    "kind='youtube#videoCategory', snippet=Row(assignable=True, channelId='UCBR8-60-B28hp2BmDPdntcQ', title='Music')\n",
    "), Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/HwXKamM1Q20q9BN-oBJavSGkfDI\"', id='15', kind='youtube#videoCategory',\n",
    "snippet=Row(assignable=True, channelId='UCBR8-60-B28hp2BmDPdntcQ', title='Pets & Animals')),\n",
    "Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/9GQMSRjrZdHeb1OEM1XVQ9zbGec\"', id='17', kind='youtube#videoCategory', \n",
    "snippet=Row(assignable=True, channelId='UCBR8-60-B28hp2BmDPdntcQ', title='Sports')), \n",
    "Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/FJwVpGCVZ1yiJrqZbpqe68Sy_OE\"', id='18', kind='youtube#videoCategory', \n",
    "snippet=Row(assignable=False, channelId='UCBR8-60-B28hp2BmDPdntcQ', title='Short Movies')), \n",
    "Row(etag='\"ld9biNPKjAjgjV7EZ4EKeEGrhao/M-3iD9dwK7YJCafRf_DkLN8CouA\"', id='19', kind='youtube#videoCategory', \n",
    "snippet=Row(assignable=True, channelId='UCBR8-60-B28hp2BmDPdntcQ', title='Travel & Events'))\"\"\"\n",
    "\n",
    "# There are many issues with the data such as for each items we have an row item but for every item we have \n",
    "# snippet=row whcih contains the title which we have to use also for our pipeline.\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, Since the etag column is not in proper format which will restrict us to find the load and use the json \n",
    "# Module therefore we will try to find the replace the etag into structured format\n",
    "# input \"\\\"ld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\\\"\"\n",
    "# output \\\"ld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\\\"\n",
    "import regex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [116], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m json_dict\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml,\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,json_obj)\n\u001b[0;32m----> 4\u001b[0m json_load\u001b[38;5;241m=\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m json_dict\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m(),\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = '\"\\\"ld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\\\"\"'\n",
    "json_dict=re.sub(r'[\"l,\\\"]',\"\",json_obj)\n",
    "json_load=json.load(json_dict)\n",
    "json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'json_location': True, 'csv_location': True, 'validated_file_location': False}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_parameters(dict_name):\n",
    "    import os\n",
    "    import regex\n",
    "    status_dict={}\n",
    "    regex_patters=re.compile(\"^.*_location\")\n",
    "    for key,value in dict_name.items():\n",
    "        if check_file_exists(key):\n",
    "            if os.path.exists(value):\n",
    "                status_dict[key]=True\n",
    "            else:\n",
    "                status_dict[key]=False\n",
    "    return status_dict\n",
    "            \n",
    "        \n",
    "        \n",
    "check_parameters(validation_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exists(string):\n",
    "    reg_pattern=re.compile(r\"[a-zA-Z,_)]+_location\")\n",
    "    return bool(re.match(reg_pattern,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_para={\n",
    "    'banner':\"DE\",\n",
    "    \"json_location\":\"/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json\",\n",
    "    \"csv_location\":\"/home/aryan/Music/YouTube_Analysis_Data/DEvideos.csv\",\n",
    "    \"table_name\":\"dedata\",\n",
    "    \"validated_file_location\":\"/home/aryan/data_pipeline/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import datetime as dt\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "logging.basicConfig(level=logging.INFO,filename=\"log.txt\")\n",
    "from googletrans import Translator\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/3824122613.py:157: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"The Pipeline to ingest data is being started\")\n",
    "logging.info(\"Checking the input parameters\")\n",
    "if len(validation_para)!=5:\n",
    "    logging.info(\"Pipeline Failed! As the parameters some of the parameters are missing\")\n",
    "else:\n",
    "    logging.info(\"Checking at these location if files are present or not!\")\n",
    "    file_location_status_dict=check_parameters(validation_para)\n",
    "    for k,v in file_location_status_dict.items():\n",
    "        if k!='validated_file_location' and v==False:\n",
    "            logging.info(f'The location of {k} is not valid, please check')\n",
    "    logging.info(\"All files have valid location and they exists.\")\n",
    "        \n",
    "    logging.info(\"Loading the json file and extracting the required fields\")\n",
    "    json_data_dict=read_json(validation_para[\"json_location\"])\n",
    "    logging.info(\"Loading the catgory names along with Ids\")\n",
    "    category_names_id_dict=getting_category_name(json_data_dict)\n",
    "    logging.info(f'Loading the Csv files from {validation_para[\"csv_location\"]}')\n",
    "    data_csv=pd.read_csv(validation_para['csv_location'])\n",
    "    data_csv=data_csv.sort_values(\"likes\",ascending=False)\n",
    "    cat_ids=data_csv.category_id.unique()\n",
    "    logging.info(\"Mapping the category names with the category ids\")\n",
    "    data_csv[\"category_Name\"]=data_csv[\"category_id\"].map(category_names_id_dict)\n",
    "    logging.info(\"Extracting the date from publish time and adding it to the same columns\")\n",
    "    data_csv[\"publish_time\"]=data_csv.publish_time.apply(lambda x:convert_datetime_to_date(x))\n",
    "    data_csv[\"publish_time\"] = data_csv[\"publish_time\"].astype(\"datetime64[ns]\")\n",
    "    logging.info(\"Conveting the datetype from string to date\")\n",
    "    data_csv.trending_date=data_csv.trending_date.map(lambda x: convert_to_date(x))\n",
    "    logging.info(\"Converting the datetime for trending date\")\n",
    "    data_csv[\"trending_date\"]=data_csv.trending_date.astype(\"datetime64[ns]\")\n",
    "    logging.info(\"Calculating the average days differnece between the trending date and publish date\")\n",
    "    data_csv[\"day_diff\"]=abs(data_csv.trending_date-data_csv.publish_time)\n",
    "    logging.info(\"Let us calculate the average earning for the vedios depending on the vedios\")\n",
    "    data_csv[\"total_income($)\"]=data_csv.views.map(lambda x : x/1000)\n",
    "    # Creating a seperate CSV files for the collecting the Average Income per Category\n",
    "    # Let us find the aggregated income of each category\n",
    "    data_cat_income=pd.DataFrame(data_csv.groupby(\"category_Name\")[\"total_income($)\"].sum().to_frame(name=' Total Income $').reset_index())\n",
    "    # Since the Required Directory is not present therefore we will try to create the directory if it does not exist\n",
    "  \n",
    "\n",
    "    validated_file_location=validation_para[\"validated_file_location\"]+validation_para[\"banner\"]+\"/Validated/Succesful/\"\n",
    "\n",
    "    if os.path.exists(validated_file_location):\n",
    "        logging.info(\"Since the file directorys is already present so no need to create the directory locally\")\n",
    "    else:\n",
    "        cmd=f'mkdir -p {validated_file_location}'\n",
    "        os.system(cmd)\n",
    "\n",
    "    logging.info(f\"Saving the Income file at{validated_file_location}/data_cat_income\")\n",
    "    data_cat_income.to_csv(f'{validated_file_location}/data_cat_income.csv')\n",
    "    \n",
    "    # Since the Api is not responding so the above code is commented out.[Still In development process]\n",
    "#     top_channels[\"channel_title(English)\"]=top_channels.channel_title.map(lambda x: translate_to_english(x))\n",
    "#     top_channels[\"channel_title\"].map(lambda x:key_generation(x))\n",
    "#     # Using this function we can create a dictionary using two column which are one normally names and other name locally.\n",
    "#     mydict = dict(zip(top_channels.channel_title,top_channels[\"channel_title(English)\"]))\n",
    "#     data_csv[\"channel_title_english\"]=data_csv.channel_title.map(lambda x: return_english_name(x,mydict))\n",
    "#     data_csv=get_columns(data_csv,\"channel_title_english\",4)\n",
    "#     # If we use this function on this completely we can get the category tags like ['BIGHIT', 'Big Hit', 'BTS', 'BTS', 'BANGTAN', 'Bangtan', 'FAK'] what we will do then\n",
    "#     # is fining the count of unique_tags and iterating it from list.\n",
    "\n",
    "#     data_csv[\"parent_Country\"]=data_csv.tags.map(lambda x: trans_to_eng_sep(x))\n",
    "#     codes_dict=get_language_codes_from_url(base_url,tags)\n",
    "#     try:\n",
    "#         data_csv[\"lang_code\"]=data_csv.parent_Country.map(lambda x: get_lang_from_code(x,codes_dict))\n",
    "#         data_csv[\"tags_eng\"]=data_csv.parent_Country.map(lambda x: get_tags_from_code(x))\n",
    "#         data_csv.drop(columns=\"parent_Country\")\n",
    "#     except:\n",
    "#         print(\"Since the code is run for one time therfore the column parent_Country has been droppend\")\n",
    "#     lang_codes=data_csv.lang_code.unique().tolist()\n",
    "#     data_csv[\"video_language\"]=data_csv.lang_code.map(lambda x:get_lang_from_codes(x,codes_dict))\n",
    "    \n",
    "#     try:\n",
    "#         data_csv=data_csv.drop(columns=\"lang_code\")\n",
    "#         print(\"Congratulation! Columns has been dropped\")\n",
    "#     except:\n",
    "#         print(\"The column lang_codes has already been dropped so no ne columns is available\")\n",
    "        \n",
    "#     \"\"\"Since we have different tags assosciated with the channel let us find the most common tags for a \n",
    "# trending youtube vedio and finding some of the values of finding the values \"\"\"\n",
    "#     # Let us define a fucnton which will converts the tags into a dictionary \n",
    "#     data_csv[\"Updated_tags\"]=data_csv.tags_eng.map(lambda x: convert_tags_to_dict(x))\n",
    "    \n",
    "    # Saving the Updated_final_data\n",
    "    final_file_location=validated_file_location+'final_validated.csv'\n",
    "    logging.info(f\"Saving the final CSV File in the location {final_file_location}\")\n",
    "    data_csv.to_csv(final_file_location)\n",
    "    \n",
    "\n",
    "    # Saving the final validation data to postgres core DB\n",
    "    conn_string=create_conn_string('postgres','9760869634','localhost','test')\n",
    "    insertion_to_sql(conn_string,data_csv,validation_para['table_name'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "cmd=f'mkdir -p {validated_file_location}'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program to read\n",
    "# json file\n",
    "json_file_location=\"/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json\"\n",
    "def read_json(json_file_location):\n",
    "    import json\n",
    "    # Opening JSON file\n",
    "    f = open('/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json')\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def getting_category_name(data_dict):\n",
    " \n",
    "    b={}\n",
    "    for i in range(len(data_dict[\"items\"])):\n",
    "        id=int(data_dict[\"items\"][i][\"id\"])\n",
    "        #print(type(id))\n",
    "        value=data_dict[\"items\"][i][\"snippet\"][\"title\"]\n",
    "        if \"id\" not in b.keys():\n",
    "            b[id]=value        \n",
    "    return b\n",
    "\n",
    "def convert_datetime_to_date(datetime_series):\n",
    "    import re\n",
    "    regex_pattern=re.compile(f\"[\\d]+-[\\d]+-[\\d]+\")\n",
    "    res=str(re.findall(regex_pattern,datetime_series))\n",
    "    date=\"\".join(res[2:12])\n",
    "    return date\n",
    "\n",
    "def check_parameters(dict_name):\n",
    "    import os\n",
    "    import regex\n",
    "    status_dict={}\n",
    "    regex_patters=re.compile(\"^.*_location\")\n",
    "    for key,value in dict_name.items():\n",
    "        if check_file_exists(key):\n",
    "            if os.path.exists(value):\n",
    "                status_dict[key]=True\n",
    "            else:\n",
    "                status_dict[key]=False\n",
    "    return status_dict\n",
    "\n",
    "def convert_to_date(data):\n",
    "    res=data.split(\".\")\n",
    "    new_pattern=\"20{}-{}-{}\".format(res[0],res[2],res[1])\n",
    "    return new_pattern\n",
    "\n",
    "\n",
    "\n",
    "def translate_to_english(text):\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    res=translator.translate(text, dest='en')\n",
    "    #print(type(res))\n",
    "    #print(res.text)\n",
    "    return res.text\n",
    "\n",
    "def key_generation(x):\n",
    "    b={}\n",
    "    if x not in b.keys():\n",
    "        b[x]=\"\"\n",
    "    return b\n",
    "\n",
    "def return_english_name(name,mydict):\n",
    "    return mydict[name]\n",
    "\n",
    "def get_columns(dataframe,column_name,required_position):\n",
    "    index_cols=dataframe.columns.to_list()\n",
    "    print(index_cols)\n",
    "    index=len(index_cols)-1\n",
    "    print(index)\n",
    "    temp_index=index_cols[:required_position]+[column_name]+index_cols[required_position:len(index_cols)-1]\n",
    "    dataframe=dataframe[temp_index]\n",
    "    return dataframe\n",
    "\n",
    "def trans_to_eng_sep(text):   \n",
    "    res=translator.translate(text, dest='en')\n",
    "    #print(type(res))\n",
    "    #print(res.text)\n",
    "    pa_lang=res.src\n",
    "    trans_text=res.text.replace('\"',\"\")\n",
    "    #print(trans_text)\n",
    "    tags=list(trans_text.split(\"|\"))\n",
    "    #print(pa_lang)\n",
    "    return  pa_lang\n",
    "\n",
    "def get_language_codes_from_url(base_url,tags):\n",
    "    \n",
    "    #regex_for_name=re.compile(\"https://www.familyeducation.com/baby-names/name-meaning/[a-z]+\")\n",
    "    common_last_names=[]\n",
    "    new_dict={}\n",
    "    req=requests.get(base_url)\n",
    "    soup=BeautifulSoup(req.content,\"html.parser\")\n",
    "    li = soup.find_all(tags)\n",
    "    for word in li:\n",
    "        words_list=word.get_text()\n",
    "        common_last_names.append(words_list)\n",
    "    \n",
    "    # We have to segregate the even and odd names as even names \n",
    "    i=0\n",
    "    while i<len(common_last_names):\n",
    "        new_dict[common_last_names[i+1]]=common_last_names[i]\n",
    "        i=i+2\n",
    "    return new_dict\n",
    "\n",
    "# From codes_dict we can create a function which takes code as input and after chencking from dictionary , it can\n",
    "# return the language as outut\n",
    "def get_lang_from_code(pandas_Series,code_dict):\n",
    "    code=pandas_Series[0]\n",
    "    return code\n",
    "\n",
    "def get_tags_from_code(pandas_Series):\n",
    "    code=pandas_Series[1]\n",
    "    return code\n",
    "\n",
    "def non_lang_codes(lang_codes,codes_dict):\n",
    "    non_added_keys=[]\n",
    "    for lang in lang_codes:\n",
    "        try:\n",
    "            if codes_dict[lang]:\n",
    "                pass\n",
    "        except:\n",
    "            non_added_keys.append(lang)\n",
    "# codes_dict[\"jw\"]=\"javanese\"\n",
    "# codes_dict[\"zh-CN\"]=\"Chinese (PRC)\"\n",
    "\n",
    "def get_lang_from_codes(codes,codes_dict):\n",
    "    return codes_dict[codes]\n",
    "\n",
    "def convert_tags_to_dict(tags_array):\n",
    "    global max_v\n",
    "    \n",
    "    tags_count_dict={}\n",
    "    for i in tags_array:\n",
    "        if i in tags_count_dict.keys():\n",
    "            tags_count_dict[i.lower()]+=1\n",
    "        else:\n",
    "            tags_count_dict[i.lower()]=1\n",
    "    if max_value<len(tags_count_dict):\n",
    "        max_value=len(tags_count_dict)\n",
    "    #print(len(tags_count_dict))\n",
    "    return tags_count_dict\n",
    "\n",
    "\n",
    "#  FUnctions for Pushing the data to Core DB\n",
    "\n",
    "def create_conn_string(username,password,host,database_name):\n",
    "    conn_string=f\"postgresql://{username}:{password}@{host}/{database_name}\"\n",
    "    return conn_string\n",
    "\n",
    "# Function to push data to a database from\n",
    "def insertion_to_sql(conn_string,dataframe_name,table_name):\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine\n",
    "    db = create_engine(conn_string)\n",
    "    conn = db.connect()\n",
    "    dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n",
    "    \n",
    "def check_data_inserted(table_name,conn_string):\n",
    "    conn=psycopg2.connect(conn_string)\n",
    "    conn.autocommit=True\n",
    "    cursor=conn.cursor()\n",
    "    sql1=f\"select * from {table_name};\"\n",
    "    cursor.execute(sql1)\n",
    "    res=cursor.fetchall()\n",
    "    print(res.head(10))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [141], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m json_string\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoutube#videoCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m   \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massignable\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: true\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m   }\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 10\u001b[0m json_file\u001b[38;5;241m=\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m json_file\n",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "json_string= '''{\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"ld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\\\"\",\n",
    "   \"id\": \"44\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Trailers\",\n",
    "    \"assignable\": true\n",
    "   }'''\n",
    "json_file=json.dump(json_string)\n",
    "json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LgVi6y5QIjM</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Sing zu Ende! | Gesangseinlagen vom Feinsten |...</td>\n",
       "      <td>inscope21</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T17:08:49.000Z</td>\n",
       "      <td>inscope21|\"sing zu ende\"|\"gesangseinlagen\"|\"ge...</td>\n",
       "      <td>252786</td>\n",
       "      <td>35885</td>\n",
       "      <td>230</td>\n",
       "      <td>1539</td>\n",
       "      <td>https://i.ytimg.com/vi/LgVi6y5QIjM/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Heute gibt es mal wieder ein neues Format... w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayt7uQith4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Kinder ferngesteuert im Kiosk! Erwachsene abzo...</td>\n",
       "      <td>LUKE! Die Woche und ich</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T22:30:01.000Z</td>\n",
       "      <td>Kinder|\"ferngesteuert\"|\"Kinder ferngesteuert\"|...</td>\n",
       "      <td>797196</td>\n",
       "      <td>53576</td>\n",
       "      <td>302</td>\n",
       "      <td>1278</td>\n",
       "      <td>https://i.ytimg.com/vi/Bayt7uQith4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Kinder ferngesteuert! Kinder lassen sich sooo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97190</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHtypnRk7JE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Das Fermi-Paradoxon</td>\n",
       "      <td>100SekundenPhysik</td>\n",
       "      <td>27</td>\n",
       "      <td>2017-11-12T15:00:01.000Z</td>\n",
       "      <td>Physik|\"Wissenschaft\"|\"Technik\"|\"Science-Ficti...</td>\n",
       "      <td>380247</td>\n",
       "      <td>31821</td>\n",
       "      <td>458</td>\n",
       "      <td>1955</td>\n",
       "      <td>https://i.ytimg.com/vi/AHtypnRk7JE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>►Alle Videos: http://bit.ly/1fa7Tw3\\n\\n\\n✚Snap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZJ9We4bjcg0</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>18 SONGS mit Kelly MissesVlog (Sing-off)</td>\n",
       "      <td>rezo</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12T13:10:36.000Z</td>\n",
       "      <td>kelly|\"missesvlog\"|\"kelly song\"|\"bausa\"|\"bausa...</td>\n",
       "      <td>822213</td>\n",
       "      <td>100684</td>\n",
       "      <td>2467</td>\n",
       "      <td>10244</td>\n",
       "      <td>https://i.ytimg.com/vi/ZJ9We4bjcg0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>18 Song Mashup über den (veränderten) Beat von...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40835</th>\n",
       "      <td>fn5WNxy-Wcw</td>\n",
       "      <td>18.14.06</td>\n",
       "      <td>KINGDOM HEARTS III – E3 2018 Pirates of the Ca...</td>\n",
       "      <td>Kingdom Hearts</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-06-12T01:54:02.000Z</td>\n",
       "      <td>Kingdom Hearts|\"KH3\"|\"Kingdom Hearts 3\"|\"Pirat...</td>\n",
       "      <td>1394530</td>\n",
       "      <td>46778</td>\n",
       "      <td>501</td>\n",
       "      <td>9878</td>\n",
       "      <td>https://i.ytimg.com/vi/fn5WNxy-Wcw/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Find out more about Kingdom Hearts 3: https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40836</th>\n",
       "      <td>zAFv43lxqHE</td>\n",
       "      <td>18.14.06</td>\n",
       "      <td>YMS: The Visit</td>\n",
       "      <td>YourMovieSucksDOTorg</td>\n",
       "      <td>24</td>\n",
       "      <td>2018-06-13T21:58:43.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>139733</td>\n",
       "      <td>11155</td>\n",
       "      <td>119</td>\n",
       "      <td>1968</td>\n",
       "      <td>https://i.ytimg.com/vi/zAFv43lxqHE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Patreon: http://www.patreon.com/YMSTwitch: htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40837</th>\n",
       "      <td>zSXG5I6Y2fA</td>\n",
       "      <td>18.14.06</td>\n",
       "      <td>Ungut umgeschult – Grünwald als Ersthelfer am ...</td>\n",
       "      <td>Grünwald Freitagscomedy</td>\n",
       "      <td>24</td>\n",
       "      <td>2018-06-12T10:01:28.000Z</td>\n",
       "      <td>Günter Grünwald|\"Grünwald Freitagscomedy\"|\"Gün...</td>\n",
       "      <td>26054</td>\n",
       "      <td>364</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>https://i.ytimg.com/vi/zSXG5I6Y2fA/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Günter versucht sich als Ersthelfer bei einem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40838</th>\n",
       "      <td>5d115sePmaU</td>\n",
       "      <td>18.14.06</td>\n",
       "      <td>Assassin's Creed Odyssey: E3 2018 Welt-Enthüll...</td>\n",
       "      <td>Assassin's Creed DE</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-06-11T21:16:55.000Z</td>\n",
       "      <td>Assassin's Creed|\"Assassins Creed\"|\"Assassin's...</td>\n",
       "      <td>1139198</td>\n",
       "      <td>14900</td>\n",
       "      <td>1421</td>\n",
       "      <td>1587</td>\n",
       "      <td>https://i.ytimg.com/vi/5d115sePmaU/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Vom verstoßenen Söldner zum legendären Helden,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40839</th>\n",
       "      <td>go-F6xvezAM</td>\n",
       "      <td>18.14.06</td>\n",
       "      <td>Гироскутер - Азбука Уральских Пельменей Б - Ур...</td>\n",
       "      <td>Уральские Пельмени</td>\n",
       "      <td>23</td>\n",
       "      <td>2018-06-13T15:02:15.000Z</td>\n",
       "      <td>Гироскутер|\"уральские пельмени гироскутер\"|\"мя...</td>\n",
       "      <td>316328</td>\n",
       "      <td>11394</td>\n",
       "      <td>352</td>\n",
       "      <td>550</td>\n",
       "      <td>https://i.ytimg.com/vi/go-F6xvezAM/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Популярный номер из нового шоу Азбука Уральски...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40840 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id trending_date  \\\n",
       "0      LgVi6y5QIjM      17.14.11   \n",
       "1      Bayt7uQith4      17.14.11   \n",
       "2      1ZAPwfrtAFY      17.14.11   \n",
       "3      AHtypnRk7JE      17.14.11   \n",
       "4      ZJ9We4bjcg0      17.14.11   \n",
       "...            ...           ...   \n",
       "40835  fn5WNxy-Wcw      18.14.06   \n",
       "40836  zAFv43lxqHE      18.14.06   \n",
       "40837  zSXG5I6Y2fA      18.14.06   \n",
       "40838  5d115sePmaU      18.14.06   \n",
       "40839  go-F6xvezAM      18.14.06   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Sing zu Ende! | Gesangseinlagen vom Feinsten |...   \n",
       "1      Kinder ferngesteuert im Kiosk! Erwachsene abzo...   \n",
       "2      The Trump Presidency: Last Week Tonight with J...   \n",
       "3                                    Das Fermi-Paradoxon   \n",
       "4               18 SONGS mit Kelly MissesVlog (Sing-off)   \n",
       "...                                                  ...   \n",
       "40835  KINGDOM HEARTS III – E3 2018 Pirates of the Ca...   \n",
       "40836                                     YMS: The Visit   \n",
       "40837  Ungut umgeschult – Grünwald als Ersthelfer am ...   \n",
       "40838  Assassin's Creed Odyssey: E3 2018 Welt-Enthüll...   \n",
       "40839  Гироскутер - Азбука Уральских Пельменей Б - Ур...   \n",
       "\n",
       "                 channel_title  category_id              publish_time  \\\n",
       "0                    inscope21           24  2017-11-13T17:08:49.000Z   \n",
       "1      LUKE! Die Woche und ich           23  2017-11-12T22:30:01.000Z   \n",
       "2              LastWeekTonight           24  2017-11-13T07:30:00.000Z   \n",
       "3            100SekundenPhysik           27  2017-11-12T15:00:01.000Z   \n",
       "4                         rezo           24  2017-11-12T13:10:36.000Z   \n",
       "...                        ...          ...                       ...   \n",
       "40835           Kingdom Hearts           20  2018-06-12T01:54:02.000Z   \n",
       "40836     YourMovieSucksDOTorg           24  2018-06-13T21:58:43.000Z   \n",
       "40837  Grünwald Freitagscomedy           24  2018-06-12T10:01:28.000Z   \n",
       "40838      Assassin's Creed DE           20  2018-06-11T21:16:55.000Z   \n",
       "40839       Уральские Пельмени           23  2018-06-13T15:02:15.000Z   \n",
       "\n",
       "                                                    tags    views   likes  \\\n",
       "0      inscope21|\"sing zu ende\"|\"gesangseinlagen\"|\"ge...   252786   35885   \n",
       "1      Kinder|\"ferngesteuert\"|\"Kinder ferngesteuert\"|...   797196   53576   \n",
       "2      last week tonight trump presidency|\"last week ...  2418783   97190   \n",
       "3      Physik|\"Wissenschaft\"|\"Technik\"|\"Science-Ficti...   380247   31821   \n",
       "4      kelly|\"missesvlog\"|\"kelly song\"|\"bausa\"|\"bausa...   822213  100684   \n",
       "...                                                  ...      ...     ...   \n",
       "40835  Kingdom Hearts|\"KH3\"|\"Kingdom Hearts 3\"|\"Pirat...  1394530   46778   \n",
       "40836                                             [none]   139733   11155   \n",
       "40837  Günter Grünwald|\"Grünwald Freitagscomedy\"|\"Gün...    26054     364   \n",
       "40838  Assassin's Creed|\"Assassins Creed\"|\"Assassin's...  1139198   14900   \n",
       "40839  Гироскутер|\"уральские пельмени гироскутер\"|\"мя...   316328   11394   \n",
       "\n",
       "       dislikes  comment_count  \\\n",
       "0           230           1539   \n",
       "1           302           1278   \n",
       "2          6146          12703   \n",
       "3           458           1955   \n",
       "4          2467          10244   \n",
       "...         ...            ...   \n",
       "40835       501           9878   \n",
       "40836       119           1968   \n",
       "40837        11              8   \n",
       "40838      1421           1587   \n",
       "40839       352            550   \n",
       "\n",
       "                                       thumbnail_link  comments_disabled  \\\n",
       "0      https://i.ytimg.com/vi/LgVi6y5QIjM/default.jpg              False   \n",
       "1      https://i.ytimg.com/vi/Bayt7uQith4/default.jpg              False   \n",
       "2      https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg              False   \n",
       "3      https://i.ytimg.com/vi/AHtypnRk7JE/default.jpg              False   \n",
       "4      https://i.ytimg.com/vi/ZJ9We4bjcg0/default.jpg              False   \n",
       "...                                               ...                ...   \n",
       "40835  https://i.ytimg.com/vi/fn5WNxy-Wcw/default.jpg              False   \n",
       "40836  https://i.ytimg.com/vi/zAFv43lxqHE/default.jpg              False   \n",
       "40837  https://i.ytimg.com/vi/zSXG5I6Y2fA/default.jpg              False   \n",
       "40838  https://i.ytimg.com/vi/5d115sePmaU/default.jpg              False   \n",
       "40839  https://i.ytimg.com/vi/go-F6xvezAM/default.jpg              False   \n",
       "\n",
       "       ratings_disabled  video_error_or_removed  \\\n",
       "0                 False                   False   \n",
       "1                 False                   False   \n",
       "2                 False                   False   \n",
       "3                 False                   False   \n",
       "4                 False                   False   \n",
       "...                 ...                     ...   \n",
       "40835             False                   False   \n",
       "40836             False                   False   \n",
       "40837             False                   False   \n",
       "40838             False                   False   \n",
       "40839             False                   False   \n",
       "\n",
       "                                             description  \n",
       "0      Heute gibt es mal wieder ein neues Format... w...  \n",
       "1      Kinder ferngesteuert! Kinder lassen sich sooo ...  \n",
       "2      One year after the presidential election, John...  \n",
       "3      ►Alle Videos: http://bit.ly/1fa7Tw3\\n\\n\\n✚Snap...  \n",
       "4      18 Song Mashup über den (veränderten) Beat von...  \n",
       "...                                                  ...  \n",
       "40835  Find out more about Kingdom Hearts 3: https://...  \n",
       "40836  Patreon: http://www.patreon.com/YMSTwitch: htt...  \n",
       "40837  Günter versucht sich als Ersthelfer bei einem ...  \n",
       "40838  Vom verstoßenen Söldner zum legendären Helden,...  \n",
       "40839  Популярный номер из нового шоу Азбука Уральски...  \n",
       "\n",
       "[40840 rows x 16 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_para[\"json_location\"]="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir='/home/aryan/Music/YouTube_Analysis_Data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JPvideos.csv',\n",
       " 'IN_category_id.json',\n",
       " 'GB_category_id.json',\n",
       " 'DE_category_id.json',\n",
       " 'MXvideos.csv',\n",
       " 'GBvideos.csv',\n",
       " 'MX_category_id.json',\n",
       " 'USvideos.csv',\n",
       " 'INvideos.csv',\n",
       " 'KRvideos.csv',\n",
       " 'CA_category_id.json',\n",
       " 'US_category_id.json',\n",
       " 'RU_category_id.json',\n",
       " 'KR_category_id.json',\n",
       " 'JP_category_id.json',\n",
       " 'FR_category_id.json',\n",
       " 'RUvideos.csv',\n",
       " 'CAvideos.csv',\n",
       " 'FRvideos.csv',\n",
       " 'DEvideos.csv']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "filenames=os.listdir(input_dir)\n",
    "filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here our idea is to extract the filenames which contains a json file and a csv file.\n",
    "\n",
    "# Designing a function to extract a json file.\n",
    "\n",
    "def generate_configs():\n",
    "    import os\n",
    "    import regex\n",
    "    banner_names=[]\n",
    "    file_info_csv={}\n",
    "    file_info_json={}\n",
    "    final_dict={}\n",
    "    filenames=os.listdir(input_dir)\n",
    "    for i in filenames:\n",
    "\n",
    "        bann_name=i[0:2]\n",
    "        reg_pattern_csv=f'{bann_name}.*.csv$'\n",
    "        reg_pattern_json=f'{bann_name}.*.json$'\n",
    "        if bool(re.findall(reg_pattern_csv,i)):\n",
    "                file_info_csv[f\"{bann_name}\"]=i\n",
    "        if bool(re.findall(reg_pattern_json,i)):\n",
    "                file_info_json[f\"{bann_name}\"]=i\n",
    "\n",
    "        if  bann_name not in banner_names:\n",
    "            banner_names.append(bann_name)\n",
    "            \n",
    "    return file_info_csv,file_info_json\n",
    "\n",
    "\n",
    "def generate_master_dict(csv_files_dict,csv_files_json):\n",
    "    master_dict={}\n",
    "    for k,v in csv_files_dict.items():\n",
    "        new_file=[]\n",
    "        new_file.append(v)\n",
    "        new_file.append(csv_files_json[k])\n",
    "        if k not in master_dict.keys():\n",
    "            master_dict[k]=new_file\n",
    "            \n",
    "    return master_dict\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IN': 'IN_category_id.json',\n",
       " 'GB': 'GB_category_id.json',\n",
       " 'DE': 'DE_category_id.json',\n",
       " 'MX': 'MX_category_id.json',\n",
       " 'CA': 'CA_category_id.json',\n",
       " 'US': 'US_category_id.json',\n",
       " 'RU': 'RU_category_id.json',\n",
       " 'KR': 'KR_category_id.json',\n",
       " 'JP': 'JP_category_id.json',\n",
       " 'FR': 'FR_category_id.json'}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files_dict,csv_files_json=generate_configs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_master_dict(csv_files_dict,csv_files_json):\n",
    "    master_dict={}\n",
    "    for k,v in csv_files_dict.items():\n",
    "        new_file=[]\n",
    "        new_file.append(v)\n",
    "        new_file.append(csv_files_json[k])\n",
    "        if k not in master_dict.keys():\n",
    "            master_dict[k]=new_file\n",
    "            \n",
    "    return master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'JP', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/JP_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/JPvideos.csv', 'table_name': 'jpdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'MX', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/MX_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/MXvideos.csv', 'table_name': 'mxdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'GB', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/GB_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/GBvideos.csv', 'table_name': 'gbdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'US', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/US_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/USvideos.csv', 'table_name': 'usdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'IN', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/IN_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/INvideos.csv', 'table_name': 'indata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'KR', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/KR_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/KRvideos.csv', 'table_name': 'krdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'RU', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/RU_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/RUvideos.csv', 'table_name': 'rudata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'CA', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/CA_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/CAvideos.csv', 'table_name': 'cadata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'FR', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/FR_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/FRvideos.csv', 'table_name': 'frdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'DE', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/DEvideos.csv', 'table_name': 'dedata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    }
   ],
   "source": [
    "master_dict=generate_master_dict(csv_files_dict,csv_files_json)\n",
    "master_dict\n",
    "\n",
    "for k,v in master_dict.items():\n",
    "    validation_para={\n",
    "    'banner':f\"{k}\",\n",
    "    \"json_location\":f\"/home/aryan/Music/YouTube_Analysis_Data/{v[1]}\",\n",
    "    \"csv_location\":f\"/home/aryan/Music/YouTube_Analysis_Data/{v[0]}\",\n",
    "    \"table_name\":f\"{k.lower()}data\",\n",
    "    \"validated_file_location\":\"/home/aryan/data_pipeline/\"\n",
    "    }\n",
    "    print(validation_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program to read\n",
    "# json file\n",
    "\n",
    "def read_json(json_file_location):\n",
    "    import json\n",
    "    # Opening JSON file\n",
    "    f = open('/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json')\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def getting_category_name(data_dict):\n",
    " \n",
    "    b={}\n",
    "    for i in range(len(data_dict[\"items\"])):\n",
    "        id=int(data_dict[\"items\"][i][\"id\"])\n",
    "        #print(type(id))\n",
    "        value=data_dict[\"items\"][i][\"snippet\"][\"title\"]\n",
    "        if \"id\" not in b.keys():\n",
    "            b[id]=value        \n",
    "    return b\n",
    "\n",
    "def convert_datetime_to_date(datetime_series):\n",
    "    import re\n",
    "    regex_pattern=re.compile(f\"[\\d]+-[\\d]+-[\\d]+\")\n",
    "    res=str(re.findall(regex_pattern,datetime_series))\n",
    "    date=\"\".join(res[2:12])\n",
    "    return date\n",
    "\n",
    "def check_parameters(dict_name):\n",
    "    import os\n",
    "    import regex\n",
    "    status_dict={}\n",
    "    regex_patters=re.compile(\"^.*_location\")\n",
    "    for key,value in dict_name.items():\n",
    "        if check_file_exists(key):\n",
    "            if os.path.exists(value):\n",
    "                status_dict[key]=True\n",
    "            else:\n",
    "                status_dict[key]=False\n",
    "    return status_dict\n",
    "\n",
    "def convert_to_date(data):\n",
    "    res=data.split(\".\")\n",
    "    new_pattern=\"20{}-{}-{}\".format(res[0],res[2],res[1])\n",
    "    return new_pattern\n",
    "\n",
    "def check_file_exists(string):\n",
    "    reg_pattern=re.compile(r\"[a-zA-Z,_)]+_location\")\n",
    "    return bool(re.match(reg_pattern,string))\n",
    "\n",
    "\n",
    "\n",
    "def translate_to_english(text):\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    res=translator.translate(text, dest='en')\n",
    "    #print(type(res))\n",
    "    #print(res.text)\n",
    "    return res.text\n",
    "\n",
    "def key_generation(x):\n",
    "    b={}\n",
    "    if x not in b.keys():\n",
    "        b[x]=\"\"\n",
    "    return b\n",
    "\n",
    "def return_english_name(name,mydict):\n",
    "    return mydict[name]\n",
    "\n",
    "def get_columns(dataframe,column_name,required_position):\n",
    "    index_cols=dataframe.columns.to_list()\n",
    "    print(index_cols)\n",
    "    index=len(index_cols)-1\n",
    "    print(index)\n",
    "    temp_index=index_cols[:required_position]+[column_name]+index_cols[required_position:len(index_cols)-1]\n",
    "    dataframe=dataframe[temp_index]\n",
    "    return dataframe\n",
    "\n",
    "def trans_to_eng_sep(text):   \n",
    "    res=Translator.translate(text, dest='en')\n",
    "    #print(type(res))\n",
    "    #print(res.text)\n",
    "    pa_lang=res.src\n",
    "    trans_text=res.text.replace('\"',\"\")\n",
    "    #print(trans_text)\n",
    "    tags=list(trans_text.split(\"|\"))\n",
    "    #print(pa_lang)\n",
    "    return  pa_lang\n",
    "\n",
    "from configs import *\n",
    "\n",
    "def get_language_codes_from_url(base_url,tags):\n",
    "    \n",
    "    #regex_for_name=re.compile(\"https://www.familyeducation.com/baby-names/name-meaning/[a-z]+\")\n",
    "    common_last_names=[]\n",
    "    new_dict={}\n",
    "    req=requests.get(base_url)\n",
    "    soup=BeautifulSoup(req.content,\"html.parser\")\n",
    "    li = soup.find_all(tags)\n",
    "    for word in li:\n",
    "        words_list=word.get_text()\n",
    "        common_last_names.append(words_list)\n",
    "    \n",
    "    # We have to segregate the even and odd names as even names \n",
    "    i=0\n",
    "    while i<len(common_last_names):\n",
    "        new_dict[common_last_names[i+1]]=common_last_names[i]\n",
    "        i=i+2\n",
    "    return new_dict\n",
    "\n",
    "# From codes_dict we can create a function which takes code as input and after chencking from dictionary , it can\n",
    "# return the language as outut\n",
    "def get_lang_from_code(pandas_Series,code_dict):\n",
    "    code=pandas_Series[0]\n",
    "    return code\n",
    "\n",
    "def get_tags_from_code(pandas_Series):\n",
    "    code=pandas_Series[1]\n",
    "    return code\n",
    "\n",
    "def non_lang_codes(lang_codes,codes_dict):\n",
    "    non_added_keys=[]\n",
    "    for lang in lang_codes:\n",
    "        try:\n",
    "            if codes_dict[lang]:\n",
    "                pass\n",
    "        except:\n",
    "            non_added_keys.append(lang)\n",
    "# codes_dict[\"jw\"]=\"javanese\"\n",
    "# codes_dict[\"zh-CN\"]=\"Chinese (PRC)\"\n",
    "\n",
    "def get_lang_from_codes(codes,codes_dict):\n",
    "    return codes_dict[codes]\n",
    "\n",
    "def convert_tags_to_dict(tags_array):\n",
    "    global max_v\n",
    "    \n",
    "    tags_count_dict={}\n",
    "    for i in tags_array:\n",
    "        if i in tags_count_dict.keys():\n",
    "            tags_count_dict[i.lower()]+=1\n",
    "        else:\n",
    "            tags_count_dict[i.lower()]=1\n",
    "    if max_value<len(tags_count_dict):\n",
    "        max_value=len(tags_count_dict)\n",
    "    #print(len(tags_count_dict))\n",
    "    return tags_count_dict\n",
    "\n",
    "\n",
    "#  FUnctions for Pushing the data to Core DB\n",
    "\n",
    "def create_conn_string(username,password,host,database_name):\n",
    "    conn_string=f\"postgresql://{username}:{password}@{host}/{database_name}\"\n",
    "    return conn_string\n",
    "\n",
    "# Function to push data to a database from\n",
    "def insertion_to_sql(conn_string,dataframe_name,table_name):\n",
    "    import pandas as pd\n",
    "    from sqlalchemy import create_engine\n",
    "    db = create_engine(conn_string)\n",
    "    conn = db.connect()\n",
    "    dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n",
    "    \n",
    "def check_data_inserted(table_name,conn_string):\n",
    "    conn=psycopg2.connect(conn_string)\n",
    "    conn.autocommit=True\n",
    "    cursor=conn.cursor()\n",
    "    sql1=f\"select * from {table_name};\"\n",
    "    cursor.execute(sql1)\n",
    "    res=cursor.fetchall()\n",
    "    print(res.head(10))\n",
    "\n",
    "\n",
    "# Here our idea is to extract the filenames which contains a json file and a csv file.\n",
    "\n",
    "# Designing a function to extract a json file.\n",
    "\n",
    "def generate_configs(input_dir):\n",
    "    import os\n",
    "    import regex\n",
    "    banner_names=[]\n",
    "    file_info_csv={}\n",
    "    file_info_json={}\n",
    "    final_dict={}\n",
    "    filenames=os.listdir(input_dir)\n",
    "    for i in filenames:\n",
    "\n",
    "        bann_name=i[0:2]\n",
    "        reg_pattern_csv=f'{bann_name}.*.csv$'\n",
    "        reg_pattern_json=f'{bann_name}.*.json$'\n",
    "        if bool(re.findall(reg_pattern_csv,i)):\n",
    "                file_info_csv[f\"{bann_name}\"]=i\n",
    "        if bool(re.findall(reg_pattern_json,i)):\n",
    "                file_info_json[f\"{bann_name}\"]=i\n",
    "\n",
    "        if  bann_name not in banner_names:\n",
    "            banner_names.append(bann_name)\n",
    "            \n",
    "    return file_info_csv,file_info_json\n",
    "\n",
    "\n",
    "def generate_master_dict(csv_files_dict,csv_files_json):\n",
    "    master_dict={}\n",
    "    for k,v in csv_files_dict.items():\n",
    "        new_file=[]\n",
    "        new_file.append(v)\n",
    "        new_file.append(csv_files_json[k])\n",
    "        if k not in master_dict.keys():\n",
    "            master_dict[k]=new_file\n",
    "            \n",
    "    return master_dict\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import datetime as dt\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "logging.basicConfig(level=logging.INFO,filename=\"log.txt\")\n",
    "from googletrans import Translator\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "logging.basicConfig(level=logging.INFO,Formatter=formatter,filename='log.txt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter the input directory/home/aryan/Music/YouTube_Analysis_Data/\n",
      "{'banner': 'JP', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/JP_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/JPvideos.csv', 'table_name': 'jpdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'MX', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/MX_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/MXvideos.csv', 'table_name': 'mxdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'GB', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/GB_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/GBvideos.csv', 'table_name': 'gbdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'US', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/US_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/USvideos.csv', 'table_name': 'usdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'IN', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/IN_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/INvideos.csv', 'table_name': 'indata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'KR', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/KR_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/KRvideos.csv', 'table_name': 'krdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'RU', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/RU_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/RUvideos.csv', 'table_name': 'rudata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'CA', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/CA_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/CAvideos.csv', 'table_name': 'cadata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'FR', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/FR_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/FRvideos.csv', 'table_name': 'frdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banner': 'DE', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/DEvideos.csv', 'table_name': 'dedata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9323/1495483293.py:163: UserWarning: the 'timedelta' type is not supported, and will be written as integer values (ns frequency) to the database.\n",
      "  dataframe_name.to_sql(table_name,con=conn,if_exists='replace',index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"The Pipeline to ingest data is being started\")\n",
    "input_file=input(\"Please Enter the input directory\")\n",
    "csv_files_dict,csv_files_json=generate_configs(input_file)\n",
    "master_dict=generate_master_dict(csv_files_dict,csv_files_json)\n",
    "for k,v in master_dict.items():\n",
    "    validation_para={\n",
    "        'banner':f\"{k}\",\n",
    "    \"json_location\":f\"/home/aryan/Music/YouTube_Analysis_Data/{v[1]}\",\n",
    "    \"csv_location\":f\"/home/aryan/Music/YouTube_Analysis_Data/{v[0]}\",\n",
    "    \"table_name\":f\"{k.lower()}data\",\n",
    "    \"validated_file_location\":\"/home/aryan/data_pipeline/\"\n",
    "    }\n",
    "    print(validation_para)\n",
    "  \n",
    "\n",
    "\n",
    "    logging.info(\"Checking the input parameters\")\n",
    "    if len(validation_para)!=5:\n",
    "        logging.info(\"Pipeline Failed! As the parameters some of the parameters are missing\")\n",
    "    else:\n",
    "        logging.info(\"Checking at these location if files are present or not!\")\n",
    "        file_location_status_dict=check_parameters(validation_para)\n",
    "        for k,v in file_location_status_dict.items():\n",
    "            if k!='validated_file_location' and v==False:\n",
    "                logging.info(f'The location of {k} is not valid, please check')\n",
    "        logging.info(\"All files have valid location and they exists.\")\n",
    "            \n",
    "        logging.info(\"Loading the json file and extracting the required fields\")\n",
    "        json_data_dict=read_json(validation_para[\"json_location\"])\n",
    "        logging.info(\"Loading the catgory names along with Ids\")\n",
    "        category_names_id_dict=getting_category_name(json_data_dict)\n",
    "        logging.info(f'Loading the Csv files from {validation_para[\"csv_location\"]}')\n",
    "        data_csv=pd.read_csv(validation_para['csv_location'],encoding='latin-1')\n",
    "        data_csv=data_csv.sort_values(\"likes\",ascending=False)\n",
    "        cat_ids=data_csv.category_id.unique()\n",
    "        logging.info(\"Mapping the category names with the category ids\")\n",
    "        data_csv[\"category_Name\"]=data_csv[\"category_id\"].map(category_names_id_dict)\n",
    "        logging.info(\"Extracting the date from publish time and adding it to the same columns\")\n",
    "        data_csv[\"publish_time\"]=data_csv.publish_time.apply(lambda x:convert_datetime_to_date(x))\n",
    "        data_csv[\"publish_time\"] = data_csv[\"publish_time\"].astype(\"datetime64[ns]\")\n",
    "        logging.info(\"Conveting the datetype from string to date\")\n",
    "        data_csv.trending_date=data_csv.trending_date.map(lambda x: convert_to_date(x))\n",
    "        logging.info(\"Converting the datetime for trending date\")\n",
    "        data_csv[\"trending_date\"]=data_csv.trending_date.astype(\"datetime64[ns]\")\n",
    "        logging.info(\"Calculating the average days differnece between the trending date and publish date\")\n",
    "        data_csv[\"day_diff\"]=abs(data_csv.trending_date-data_csv.publish_time)\n",
    "        logging.info(\"Let us calculate the average earning for the vedios depending on the vedios\")\n",
    "        data_csv[\"total_income($)\"]=data_csv.views.map(lambda x : x/1000)\n",
    "        # Creating a seperate CSV files for the collecting the Average Income per Category\n",
    "        # Let us find the aggregated income of each category\n",
    "        data_cat_income=pd.DataFrame(data_csv.groupby(\"category_Name\")[\"total_income($)\"].sum().to_frame(name=' Total Income $').reset_index())\n",
    "        # Since the Required Directory is not present therefore we will try to create the directory if it does not exist\n",
    "    \n",
    "\n",
    "        validated_file_location=validation_para[\"validated_file_location\"]+validation_para[\"banner\"]+\"/Validated/Succesful/\"\n",
    "\n",
    "        if os.path.exists(validated_file_location):\n",
    "            logging.info(\"Since the file directorys is already present so no need to create the directory locally\")\n",
    "        else:\n",
    "            cmd=f'mkdir -p {validated_file_location}'\n",
    "            os.system(cmd)\n",
    "\n",
    "        logging.info(f\"Saving the Income file at{validated_file_location}/data_cat_income\")\n",
    "        data_cat_income.to_csv(f'{validated_file_location}/data_cat_income.csv')\n",
    "        \n",
    "        # Since the Api is not responding so the above code is commented out.[Still In development process]\n",
    "    #     top_channels[\"channel_title(English)\"]=top_channels.channel_title.map(lambda x: translate_to_english(x))\n",
    "    #     top_channels[\"channel_title\"].map(lambda x:key_generation(x))\n",
    "    #     # Using this function we can create a dictionary using two column which are one normally names and other name locally.\n",
    "    #     mydict = dict(zip(top_channels.channel_title,top_channels[\"channel_title(English)\"]))\n",
    "    #     data_csv[\"channel_title_english\"]=data_csv.channel_title.map(lambda x: return_english_name(x,mydict))\n",
    "    #     data_csv=get_columns(data_csv,\"channel_title_english\",4)\n",
    "    #     # If we use this function on this completely we can get the category tags like ['BIGHIT', 'Big Hit', 'BTS', 'BTS', 'BANGTAN', 'Bangtan', 'FAK'] what we will do then\n",
    "    #     # is fining the count of unique_tags and iterating it from list.\n",
    "\n",
    "    #     data_csv[\"parent_Country\"]=data_csv.tags.map(lambda x: trans_to_eng_sep(x))\n",
    "    #     codes_dict=get_language_codes_from_url(base_url,tags)\n",
    "    #     try:\n",
    "    #         data_csv[\"lang_code\"]=data_csv.parent_Country.map(lambda x: get_lang_from_code(x,codes_dict))\n",
    "    #         data_csv[\"tags_eng\"]=data_csv.parent_Country.map(lambda x: get_tags_from_code(x))\n",
    "    #         data_csv.drop(columns=\"parent_Country\")\n",
    "    #     except:\n",
    "    #         print(\"Since the code is run for one time therfore the column parent_Country has been droppend\")\n",
    "    #     lang_codes=data_csv.lang_code.unique().tolist()\n",
    "    #     data_csv[\"video_language\"]=data_csv.lang_code.map(lambda x:get_lang_from_codes(x,codes_dict))\n",
    "        \n",
    "    #     try:\n",
    "    #         data_csv=data_csv.drop(columns=\"lang_code\")\n",
    "    #         print(\"Congratulation! Columns has been dropped\")\n",
    "    #     except:\n",
    "    #         print(\"The column lang_codes has already been dropped so no ne columns is available\")\n",
    "            \n",
    "    #     \"\"\"Since we have different tags assosciated with the channel let us find the most common tags for a \n",
    "    # trending youtube vedio and finding some of the values of finding the values \"\"\"\n",
    "    #     # Let us define a fucnton which will converts the tags into a dictionary \n",
    "    #     data_csv[\"Updated_tags\"]=data_csv.tags_eng.map(lambda x: convert_tags_to_dict(x))\n",
    "        \n",
    "        # Saving the Updated_final_data\n",
    "        final_file_location=validated_file_location+'final_validated.csv'\n",
    "        logging.info(f\"Saving the final CSV File in the location {final_file_location}\")\n",
    "        data_csv.to_csv(final_file_location)\n",
    "        \n",
    "\n",
    "        # Saving the final validation data to postgres core DB\n",
    "        conn_string=create_conn_string('postgres','9760869634','localhost','test')\n",
    "        insertion_to_sql(conn_string,data_csv,validation_para['table_name'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter the input directory/home/aryan/Music/YouTube_Analysis_Data/\n",
      "{'banner': 'JP', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/JP_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/JPvideos.csv', 'table_name': 'jpdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'MX', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/MX_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/MXvideos.csv', 'table_name': 'mxdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'GB', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/GB_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/GBvideos.csv', 'table_name': 'gbdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'US', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/US_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/USvideos.csv', 'table_name': 'usdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'IN', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/IN_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/INvideos.csv', 'table_name': 'indata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'KR', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/KR_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/KRvideos.csv', 'table_name': 'krdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'RU', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/RU_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/RUvideos.csv', 'table_name': 'rudata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'CA', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/CA_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/CAvideos.csv', 'table_name': 'cadata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'FR', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/FR_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/FRvideos.csv', 'table_name': 'frdata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'banner': 'DE', 'json_location': '/home/aryan/Music/YouTube_Analysis_Data/DE_category_id.json', 'csv_location': '/home/aryan/Music/YouTube_Analysis_Data/DEvideos.csv', 'table_name': 'dedata', 'validated_file_location': '/home/aryan/data_pipeline/'}\n",
      "{'kind': 'youtube#videoCategoryListResponse', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/1v2mrzYSYG6onNLt2qTj13hkQZk\"', 'items': [{'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/Xy1mB4_yLrHy_BmKmPBggty2mZQ\"', 'id': '1', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Film & Animation', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"', 'id': '2', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Autos & Vehicles', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/nqRIq97-xe5XRZTxbknKFVe5Lmg\"', 'id': '10', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Music', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/HwXKamM1Q20q9BN-oBJavSGkfDI\"', 'id': '15', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Pets & Animals', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/9GQMSRjrZdHeb1OEM1XVQ9zbGec\"', 'id': '17', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Sports', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/FJwVpGCVZ1yiJrqZbpqe68Sy_OE\"', 'id': '18', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Short Movies', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/M-3iD9dwK7YJCafRf_DkLN8CouA\"', 'id': '19', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Travel & Events', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/WmA0qYEfjWsAoyJFSw2zinhn2wM\"', 'id': '20', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Gaming', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/EapFaGYG7K0StIXVf8aba249tdM\"', 'id': '21', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Videoblogging', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/xId8RX7vRN8rqkbYZbNIytUQDRo\"', 'id': '22', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'People & Blogs', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/G9LHzQmx44rX2S5yaga_Aqtwz8M\"', 'id': '23', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Comedy', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/UVB9oxX2Bvqa_w_y3vXSLVK5E_s\"', 'id': '24', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Entertainment', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/QiLK0ZIrFoORdk_g2l_XR_ECjDc\"', 'id': '25', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'News & Politics', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/r6Ck6Z0_L0rG37VJQR200SGNA_w\"', 'id': '26', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Howto & Style', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/EoYkczo9I3RCf96RveKTOgOPkUM\"', 'id': '27', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Education', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/w5HjcTD82G_XA3xBctS30zS-JpQ\"', 'id': '28', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Science & Technology', 'assignable': True}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/lL7uWDr_071CHxifjYG1tJrp4Uo\"', 'id': '30', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Movies', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/WnuVfjO-PyFLO7NTRQIbrGE62nk\"', 'id': '31', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Anime/Animation', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/ctpH2hGA_UZ3volJT_FTlOg9M00\"', 'id': '32', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Action/Adventure', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/L0kR3-g1BAo5UD1PLVbQ7LkkDtQ\"', 'id': '33', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Classics', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/pUZOAC_s9sfiwar639qr_wAB-aI\"', 'id': '34', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Comedy', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/Xb5JLhtyNRN3AQq021Ds-OV50Jk\"', 'id': '35', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Documentary', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/u8WXzF4HIhtEi805__sqjuA4lEk\"', 'id': '36', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Drama', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/D04PP4Gr7wc4IV_O9G66Z4A8KWQ\"', 'id': '37', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Family', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/i5-_AceGXQCEEMWU0V8CcQm_vLQ\"', 'id': '38', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Foreign', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/rtlxd0zOixA9QHdIZB26-St5qgQ\"', 'id': '39', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Horror', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/N1TrDFLRppxZgBowCJfJCvh0Dpg\"', 'id': '40', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Sci-Fi/Fantasy', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/7UMGi6zRySqXopr_rv4sZq6Za2E\"', 'id': '41', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Thriller', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/RScXhi324h8usyIetreAVb-uKeM\"', 'id': '42', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Shorts', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/0n9MJVCDLpA8q7aiGVrFsuFsd0A\"', 'id': '43', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Shows', 'assignable': False}}, {'kind': 'youtube#videoCategory', 'etag': '\"ld9biNPKjAjgjV7EZ4EKeEGrhao/x5NxSf5fz8hn4loSN4rvhwzD_pY\"', 'id': '44', 'snippet': {'channelId': 'UCBR8-60-B28hp2BmDPdntcQ', 'title': 'Trailers', 'assignable': False}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          video_id trending_date  \\\n",
      "0      LgVi6y5QIjM      17.14.11   \n",
      "1      Bayt7uQith4      17.14.11   \n",
      "2      1ZAPwfrtAFY      17.14.11   \n",
      "3      AHtypnRk7JE      17.14.11   \n",
      "4      ZJ9We4bjcg0      17.14.11   \n",
      "...            ...           ...   \n",
      "40835  fn5WNxy-Wcw      18.14.06   \n",
      "40836  zAFv43lxqHE      18.14.06   \n",
      "40837  zSXG5I6Y2fA      18.14.06   \n",
      "40838  5d115sePmaU      18.14.06   \n",
      "40839  go-F6xvezAM      18.14.06   \n",
      "\n",
      "                                                   title  \\\n",
      "0      Sing zu Ende! | Gesangseinlagen vom Feinsten |...   \n",
      "1      Kinder ferngesteuert im Kiosk! Erwachsene abzo...   \n",
      "2      The Trump Presidency: Last Week Tonight with J...   \n",
      "3                                    Das Fermi-Paradoxon   \n",
      "4               18 SONGS mit Kelly MissesVlog (Sing-off)   \n",
      "...                                                  ...   \n",
      "40835  KINGDOM HEARTS III – E3 2018 Pirates of the Ca...   \n",
      "40836                                     YMS: The Visit   \n",
      "40837  Ungut umgeschult – Grünwald als Ersthelfer am ...   \n",
      "40838  Assassin's Creed Odyssey: E3 2018 Welt-Enthüll...   \n",
      "40839  Гироскутер - Азбука Уральских Пельменей Б - Ур...   \n",
      "\n",
      "                 channel_title  category_id              publish_time  \\\n",
      "0                    inscope21           24  2017-11-13T17:08:49.000Z   \n",
      "1      LUKE! Die Woche und ich           23  2017-11-12T22:30:01.000Z   \n",
      "2              LastWeekTonight           24  2017-11-13T07:30:00.000Z   \n",
      "3            100SekundenPhysik           27  2017-11-12T15:00:01.000Z   \n",
      "4                         rezo           24  2017-11-12T13:10:36.000Z   \n",
      "...                        ...          ...                       ...   \n",
      "40835           Kingdom Hearts           20  2018-06-12T01:54:02.000Z   \n",
      "40836     YourMovieSucksDOTorg           24  2018-06-13T21:58:43.000Z   \n",
      "40837  Grünwald Freitagscomedy           24  2018-06-12T10:01:28.000Z   \n",
      "40838      Assassin's Creed DE           20  2018-06-11T21:16:55.000Z   \n",
      "40839       Уральские Пельмени           23  2018-06-13T15:02:15.000Z   \n",
      "\n",
      "                                                    tags    views   likes  \\\n",
      "0      inscope21|\"sing zu ende\"|\"gesangseinlagen\"|\"ge...   252786   35885   \n",
      "1      Kinder|\"ferngesteuert\"|\"Kinder ferngesteuert\"|...   797196   53576   \n",
      "2      last week tonight trump presidency|\"last week ...  2418783   97190   \n",
      "3      Physik|\"Wissenschaft\"|\"Technik\"|\"Science-Ficti...   380247   31821   \n",
      "4      kelly|\"missesvlog\"|\"kelly song\"|\"bausa\"|\"bausa...   822213  100684   \n",
      "...                                                  ...      ...     ...   \n",
      "40835  Kingdom Hearts|\"KH3\"|\"Kingdom Hearts 3\"|\"Pirat...  1394530   46778   \n",
      "40836                                             [none]   139733   11155   \n",
      "40837  Günter Grünwald|\"Grünwald Freitagscomedy\"|\"Gün...    26054     364   \n",
      "40838  Assassin's Creed|\"Assassins Creed\"|\"Assassin's...  1139198   14900   \n",
      "40839  Гироскутер|\"уральские пельмени гироскутер\"|\"мя...   316328   11394   \n",
      "\n",
      "       dislikes  comment_count  \\\n",
      "0           230           1539   \n",
      "1           302           1278   \n",
      "2          6146          12703   \n",
      "3           458           1955   \n",
      "4          2467          10244   \n",
      "...         ...            ...   \n",
      "40835       501           9878   \n",
      "40836       119           1968   \n",
      "40837        11              8   \n",
      "40838      1421           1587   \n",
      "40839       352            550   \n",
      "\n",
      "                                       thumbnail_link  comments_disabled  \\\n",
      "0      https://i.ytimg.com/vi/LgVi6y5QIjM/default.jpg              False   \n",
      "1      https://i.ytimg.com/vi/Bayt7uQith4/default.jpg              False   \n",
      "2      https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg              False   \n",
      "3      https://i.ytimg.com/vi/AHtypnRk7JE/default.jpg              False   \n",
      "4      https://i.ytimg.com/vi/ZJ9We4bjcg0/default.jpg              False   \n",
      "...                                               ...                ...   \n",
      "40835  https://i.ytimg.com/vi/fn5WNxy-Wcw/default.jpg              False   \n",
      "40836  https://i.ytimg.com/vi/zAFv43lxqHE/default.jpg              False   \n",
      "40837  https://i.ytimg.com/vi/zSXG5I6Y2fA/default.jpg              False   \n",
      "40838  https://i.ytimg.com/vi/5d115sePmaU/default.jpg              False   \n",
      "40839  https://i.ytimg.com/vi/go-F6xvezAM/default.jpg              False   \n",
      "\n",
      "       ratings_disabled  video_error_or_removed  \\\n",
      "0                 False                   False   \n",
      "1                 False                   False   \n",
      "2                 False                   False   \n",
      "3                 False                   False   \n",
      "4                 False                   False   \n",
      "...                 ...                     ...   \n",
      "40835             False                   False   \n",
      "40836             False                   False   \n",
      "40837             False                   False   \n",
      "40838             False                   False   \n",
      "40839             False                   False   \n",
      "\n",
      "                                             description  \n",
      "0      Heute gibt es mal wieder ein neues Format... w...  \n",
      "1      Kinder ferngesteuert! Kinder lassen sich sooo ...  \n",
      "2      One year after the presidential election, John...  \n",
      "3      ►Alle Videos: http://bit.ly/1fa7Tw3\\n\\n\\n✚Snap...  \n",
      "4      18 Song Mashup über den (veränderten) Beat von...  \n",
      "...                                                  ...  \n",
      "40835  Find out more about Kingdom Hearts 3: https://...  \n",
      "40836  Patreon: http://www.patreon.com/YMSTwitch: htt...  \n",
      "40837  Günter versucht sich als Ersthelfer bei einem ...  \n",
      "40838  Vom verstoßenen Söldner zum legendären Helden,...  \n",
      "40839  Популярный номер из нового шоу Азбука Уральски...  \n",
      "\n",
      "[40840 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"The Pipeline to ingest data is being started\")\n",
    "input_file=input(\"Please Enter the input directory\")\n",
    "csv_files_dict,csv_files_json=generate_configs(input_file)\n",
    "master_dict=generate_master_dict(csv_files_dict,csv_files_json)\n",
    "for k,v in master_dict.items():\n",
    "    validation_para={\n",
    "        'banner':f\"{k}\",\n",
    "    \"json_location\":f\"/home/aryan/Music/YouTube_Analysis_Data/{v[1]}\",\n",
    "    \"csv_location\":f\"/home/aryan/Music/YouTube_Analysis_Data/{v[0]}\",\n",
    "    \"table_name\":f\"{k.lower()}data\",\n",
    "    \"validated_file_location\":\"/home/aryan/data_pipeline/\"\n",
    "    }\n",
    "    print(validation_para)\n",
    "logging.info(\"Checking the input parameters\")\n",
    "if len(validation_para)!=5:\n",
    "    logging.info(\"Pipeline Failed! As the parameters some of the parameters are missing\")\n",
    "else:\n",
    "    logging.info(\"Checking at these location if files are present or not!\")\n",
    "    file_location_status_dict=check_parameters(validation_para)\n",
    "    for k,v in file_location_status_dict.items():\n",
    "        if k!='validated_file_location' and v==False:\n",
    "            logging.info(f'The location of {k} is not valid, please check')\n",
    "    logging.info(\"All files have valid location and they exists.\")\n",
    "            \n",
    "    logging.info(\"Loading the json file and extracting the required fields\")\n",
    "    json_data_dict=read_json(validation_para[\"json_location\"])\n",
    "    print(json_data)\n",
    "    logging.info(\"Loading the catgory names along with Ids\")\n",
    "    category_names_id_dict=getting_category_name(json_data_dict)\n",
    "    logging.info(f'Loading the Csv files from {validation_para[\"csv_location\"]}')\n",
    "    data_csv=pd.read_csv(validation_para['csv_location'])\n",
    "    print(data_csv)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
